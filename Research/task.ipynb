{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hxc9ajg6gJ7",
        "outputId": "61f0622d-10b3-41b3-f297-b15364e1aef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting llama-index-embeddings-openai\n",
            "  Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl.metadata (635 bytes)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.0 (from llama-index-embeddings-openai)\n",
            "  Downloading llama_index_core-0.11.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting openai>=1.1.0 (from llama-index-embeddings-openai)\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.5.14)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.6.1)\n",
            "Requirement already satisfied: httpx in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (10.4.0)\n",
            "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.5.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.4.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-embeddings-openai)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->llama-index-embeddings-openai)\n",
            "  Downloading jiter-0.5.0-cp312-none-win_amd64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: sniffio in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.7)\n",
            "Requirement already satisfied: certifi in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.14.0)\n",
            "Requirement already satisfied: click in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.7.24)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.20.1 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai)\n",
            "  Downloading pydantic_core-2.20.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.0.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.4.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.22.0)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (23.2)\n",
            "Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_core-0.11.3-py3-none-any.whl (1.6 MB)\n",
            "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.6/1.6 MB 14.0 MB/s eta 0:00:00\n",
            "Downloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.5.0-cp312-none-win_amd64.whl (189 kB)\n",
            "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
            "Downloading pydantic_core-2.20.1-cp312-none-win_amd64.whl (1.9 MB)\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.9/1.9 MB 11.6 MB/s eta 0:00:00\n",
            "Downloading tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
            "   ---------------------------------------- 0.0/799.3 kB ? eta -:--:--\n",
            "   --------------------------------------- 799.3/799.3 kB 35.8 MB/s eta 0:00:00\n",
            "Downloading wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: dirtyjson, wrapt, pydantic-core, jiter, distro, annotated-types, tiktoken, pydantic, deprecated, openai, llama-index-core, llama-index-embeddings-openai\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.17\n",
            "    Uninstalling pydantic-1.10.17:\n",
            "      Successfully uninstalled pydantic-1.10.17\n",
            "Successfully installed annotated-types-0.7.0 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 jiter-0.5.0 llama-index-core-0.11.3 llama-index-embeddings-openai-0.2.3 openai-1.43.0 pydantic-2.8.2 pydantic-core-2.20.1 tiktoken-0.7.0 wrapt-1.16.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchainplus-sdk 0.0.20 requires pydantic<2,>=1, but you have pydantic 2.8.2 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install llama-index-embeddings-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "johEVUqAC5DV",
        "outputId": "ccca18eb-24ca-4076-ec08-ec7cf532e933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting llama-index-vector-stores-qdrant\n",
            "  Downloading llama_index_vector_stores_qdrant-0.3.0-py3-none-any.whl.metadata (767 bytes)\n",
            "Collecting llama-index-readers-file\n",
            "  Downloading llama_index_readers_file-0.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-embeddings-fastembed\n",
            "  Downloading llama_index_embeddings_fastembed-0.2.0-py3-none-any.whl.metadata (697 bytes)\n",
            "Collecting llama-index-llms-openai\n",
            "  Downloading llama_index_llms_openai-0.2.0-py3-none-any.whl.metadata (648 bytes)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-vector-stores-qdrant) (1.65.5)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-vector-stores-qdrant) (0.11.3)\n",
            "Collecting qdrant-client>=1.7.1 (from llama-index-vector-stores-qdrant)\n",
            "  Downloading qdrant_client-1.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-readers-file) (4.12.3)\n",
            "Collecting pandas (from llama-index-readers-file)\n",
            "  Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-readers-file) (4.3.1)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting fastembed>=0.2.2 (from llama-index-embeddings-fastembed)\n",
            "  Downloading fastembed-0.3.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-llms-openai) (1.43.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Collecting PyStemmer<3.0.0,>=2.2.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading PyStemmer-2.2.0.1.tar.gz (303 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.24.6)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting mmh3<5.0,>=4.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.26.4)\n",
            "Collecting onnx<2.0.0,>=1.15.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading onnx-1.16.2-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime<2.0.0,>=1.17.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading onnxruntime-1.19.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (10.4.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (2.32.3)\n",
            "Collecting snowballstemmer<3.0.0,>=2.2.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.19.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (0.5.14)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2024.6.1)\n",
            "Requirement already satisfied: httpx in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (3.9.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (0.9.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading grpcio_tools-1.66.1-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index-readers-file)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-readers-file)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.7)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading protobuf-5.28.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Collecting grpcio<2.0.0,>=1.60.0 (from llama-index-vector-stores-qdrant)\n",
            "  Downloading grpcio-1.66.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (73.0.1)\n",
            "Requirement already satisfied: certifi in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.15.4)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (23.2)\n",
            "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.4.6)\n",
            "Collecting win32-setctime>=1.0.0 (from loguru<0.8.0,>=0.7.2->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading win32_setctime-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: click in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2024.7.24)\n",
            "Collecting coloredlogs (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: sympy in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.13.2)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (306)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-qdrant) (3.22.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.3.0)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading llama_index_vector_stores_qdrant-0.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_readers_file-0.2.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_embeddings_fastembed-0.2.0-py3-none-any.whl (2.7 kB)\n",
            "Downloading llama_index_llms_openai-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading fastembed-0.3.6-py3-none-any.whl (55 kB)\n",
            "Downloading qdrant_client-1.11.1-py3-none-any.whl (259 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
            "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 2.4/11.5 MB 10.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 6.6/11.5 MB 16.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.2/11.5 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.5/11.5 MB 16.7 MB/s eta 0:00:00\n",
            "Downloading grpcio_tools-1.66.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
            "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.1/1.1 MB 17.6 MB/s eta 0:00:00\n",
            "Downloading grpcio-1.66.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
            "   ------------------------------- -------- 3.4/4.3 MB 18.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.3/4.3 MB 18.3 MB/s eta 0:00:00\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl (31 kB)\n",
            "Downloading onnx-1.16.2-cp312-cp312-win_amd64.whl (14.4 MB)\n",
            "   ---------------------------------------- 0.0/14.4 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 3.7/14.4 MB 18.2 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 7.3/14.4 MB 18.1 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 11.0/14.4 MB 18.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 14.4/14.4 MB 18.2 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.19.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 3.7/11.1 MB 18.2 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 7.6/11.1 MB 18.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.0/11.1 MB 18.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 18.2 MB/s eta 0:00:00\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "Downloading protobuf-5.28.0-cp310-abi3-win_amd64.whl (431 kB)\n",
            "Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
            "Building wheels for collected packages: PyStemmer\n",
            "  Building wheel for PyStemmer (pyproject.toml): started\n",
            "  Building wheel for PyStemmer (pyproject.toml): finished with status 'error'\n",
            "Failed to build PyStemmer\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for PyStemmer (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [8 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      cythoning src/Stemmer.pyx to src\\Stemmer.c\n",
            "      C:\\Windows\\Temp\\pip-build-env-qrrfc9fh\\normal\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Windows\\Temp\\pip-install-0t7d5h_u\\pystemmer_105f97b66ece4ad6bc2a163ef540e8f1\\src\\Stemmer.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      building 'Stemmer' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for PyStemmer\n",
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (PyStemmer)\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-vector-stores-qdrant llama-index-readers-file llama-index-embeddings-fastembed llama-index-llms-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YGgHfbJiuuyK",
        "outputId": "354da28d-46fe-47c0-8280-2365c56125f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting qdrant_client\n",
            "  Downloading qdrant_client-1.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fastembed\n",
            "  Downloading fastembed-0.3.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from qdrant_client) (1.65.5)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant_client)\n",
            "  Downloading grpcio_tools-1.66.1-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.26 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from qdrant_client) (1.26.4)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant_client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from qdrant_client) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from qdrant_client) (2.2.2)\n",
            "Collecting PyStemmer<3.0.0,>=2.2.0 (from fastembed)\n",
            "  Downloading PyStemmer-2.2.0.1.tar.gz (303 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed) (0.24.6)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting mmh3<5.0,>=4.0 (from fastembed)\n",
            "  Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Collecting onnx<2.0.0,>=1.15.0 (from fastembed)\n",
            "  Downloading onnx-1.16.2-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime<2.0.0,>=1.17.0 (from fastembed)\n",
            "  Downloading onnxruntime-1.19.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed) (10.4.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed) (2.32.3)\n",
            "Collecting snowballstemmer<3.0.0,>=2.2.0 (from fastembed)\n",
            "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed) (0.19.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from fastembed) (4.66.5)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant_client)\n",
            "  Downloading protobuf-5.28.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Collecting grpcio>=1.41.0 (from qdrant_client)\n",
            "  Downloading grpcio-1.66.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: setuptools in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant_client) (73.0.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.4.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.5)\n",
            "Requirement already satisfied: idna in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant_client)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
            "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (0.4.6)\n",
            "Collecting win32-setctime>=1.0.0 (from loguru<0.8.0,>=0.7.2->fastembed)\n",
            "  Downloading win32_setctime-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting coloredlogs (from onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: sympy in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (1.13.2)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant_client) (306)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from pydantic>=1.10.8->qdrant_client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from pydantic>=1.10.8->qdrant_client) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (3.3.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\victus\\envs\\chatbot\\lib\\site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed) (1.3.0)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed)\n",
            "  Downloading pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading qdrant_client-1.11.1-py3-none-any.whl (259 kB)\n",
            "Downloading fastembed-0.3.6-py3-none-any.whl (55 kB)\n",
            "Downloading grpcio_tools-1.66.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
            "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.1/1.1 MB 17.6 MB/s eta 0:00:00\n",
            "Downloading grpcio-1.66.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
            "   -------------------------- ------------- 2.9/4.3 MB 16.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.3/4.3 MB 15.1 MB/s eta 0:00:00\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "Downloading mmh3-4.1.0-cp312-cp312-win_amd64.whl (31 kB)\n",
            "Downloading onnx-1.16.2-cp312-cp312-win_amd64.whl (14.4 MB)\n",
            "   ---------------------------------------- 0.0/14.4 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 3.7/14.4 MB 19.8 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 7.1/14.4 MB 17.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 10.5/14.4 MB 17.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 13.1/14.4 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 14.4/14.4 MB 16.2 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.19.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 2.4/11.1 MB 11.2 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 5.5/11.1 MB 14.0 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 9.2/11.1 MB 15.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 15.4 MB/s eta 0:00:00\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
            "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "Downloading protobuf-5.28.0-cp310-abi3-win_amd64.whl (431 kB)\n",
            "Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
            "Building wheels for collected packages: PyStemmer\n",
            "  Building wheel for PyStemmer (pyproject.toml): started\n",
            "  Building wheel for PyStemmer (pyproject.toml): finished with status 'error'\n",
            "Failed to build PyStemmer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for PyStemmer (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [8 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      cythoning src/Stemmer.pyx to src\\Stemmer.c\n",
            "      C:\\Windows\\Temp\\pip-build-env-arwhv1f2\\normal\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Windows\\Temp\\pip-install-xc22rxd3\\pystemmer_5c911f4f25444d3d9f14561f9b8ae515\\src\\Stemmer.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      building 'Stemmer' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for PyStemmer\n",
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (PyStemmer)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U qdrant_client fastembed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ykbFFWdo_tdD",
        "outputId": "9ee35b41-b692-4c0a-b52e-70a027d02fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-llms-huggingface\n",
            "  Downloading llama_index_llms_huggingface-0.3.1-py3-none-any.whl.metadata (789 bytes)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.11.2)\n",
            "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
            "  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.32.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.2.2)\n",
            "Downloading llama_index_llms_huggingface-0.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: text-generation, llama-index-llms-huggingface\n",
            "Successfully installed llama-index-llms-huggingface-0.3.1 text-generation-0.7.0\n",
            "Collecting llama-index-llms-huggingface-api\n",
            "  Downloading llama_index_llms_huggingface_api-0.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface-api) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface-api) (0.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.2.2)\n",
            "Downloading llama_index_llms_huggingface_api-0.2.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: llama-index-llms-huggingface-api\n",
            "Successfully installed llama-index-llms-huggingface-api-0.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-huggingface\n",
        "%pip install llama-index-llms-huggingface-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J1UIGXvB_wAQ",
        "outputId": "5fc34b63-20f6-4b1d-9160-44496d8e7a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: huggingface_hub[inference] in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[inference]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[inference]) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[inference]) (3.10.5)\n",
            "Collecting minijinja>=1.0 (from huggingface_hub[inference])\n",
            "  Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface_hub[inference]) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (861 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minijinja\n",
            "Successfully installed minijinja-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoGMEXkXwNhd",
        "outputId": "08c6bfec-1b68-4f1c-d2e0-68537ce22246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "collections=[]\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"e6339a43-2f5e-43cd-ae15-e1564da0b119.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"PyowyU5yg-DRvxUYTWMJsMSPYRUCUBFa8ZO_wwhCaIUSJiEoZ3Cv2Q\",\n",
        ")\n",
        "\n",
        "print(qdrant_client.get_collections())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "7428364306a6468fb233872c8ac07947",
            "f11ff3b037d04be4bdcd7c06f2bd5367",
            "c7b9b89c104d4af9998dc0836f2ac35b",
            "15739bf89beb4aee9d13554c908bcab2",
            "5fd4f58f33f64463b9dd6efad01ea8f8",
            "0353c01cd89a4065bab8461ef6b463d5",
            "ef95dfceb7ae4109ab1bccda41c1a27a",
            "b8296a056cd34b33b1b8b48072bb484d",
            "9926be4517a34cf3a04a8d17c6bdc52f",
            "3cdc47b937eb4eb29debdd9e729a49d8",
            "2a58b0f465c840ccb256c15dcaea3e0b"
          ]
        },
        "id": "rAbZgRt34Uv9",
        "outputId": "0bc41c43-5737-430e-df0a-69de19e58b37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7428364306a6468fb233872c8ac07947",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "FastEmbedEmbedding(model_name='BAAI/bge-small-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7b5e224ca710>, num_workers=None, max_length=512, cache_dir=None, threads=None, doc_embed_type='default')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "\n",
        "\n",
        "model = FastEmbedEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ns_UDxkZ0S5I"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"/content/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ltmR0n7569c"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=500, chunk_overlap=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHXUM5L34HBv"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"anxiety_vectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShTPF4sYwkSy"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=model,\n",
        "    vector_store=vector_store,\n",
        "    transformations=[splitter],\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8VfoXe3AKx_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('mobilellm')\n",
        "import os\n",
        "os.environ['HUGGING_FACE_TOKEN'] = userdata.get('mobilellm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y1Ksstiq8nCp"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "from typing import List, Optional\n",
        "remotely_run = HuggingFaceInferenceAPI(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token = userdata.get('mobilellm')\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iU_cUn1RskRk",
        "outputId": "9d6b990b-b03e-4e01-e415-193b0547e4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (0.11.2)\n",
            "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.5)\n",
            "Requirement already satisfied: minijinja>=1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.0.32)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.42.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.4.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, llama-index-embeddings-huggingface\n",
            "Successfully installed llama-index-embeddings-huggingface-0.3.1 sentence-transformers-3.0.1\n",
            "Collecting llama-index-llms-llama-cpp\n",
            "  Downloading llama_index_llms_llama_cpp-0.2.1-py3-none-any.whl.metadata (644 bytes)\n",
            "Collecting llama-cpp-python<0.3.0,>=0.2.32 (from llama-index-llms-llama-cpp)\n",
            "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-llama-cpp) (0.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (4.66.5)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (24.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-llama-cpp) (1.2.2)\n",
            "Downloading llama_index_llms_llama_cpp-0.2.1-py3-none-any.whl (6.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3398628 sha256=1602ec8173f09cb52de7634343bf65336725f9cd7d70a356fac61986017013d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python, llama-index-llms-llama-cpp\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90 llama-index-llms-llama-cpp-0.2.1\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-llms-llama-cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYZLPMxLOh8_"
      },
      "outputs": [],
      "source": [
        "# query_llm = HuggingFaceInferenceAPI(\n",
        "#     model_name=\"bofenghuang/vigogne-7b-instruct\", token = userdata.get('mobilellm')\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G5q7BIxTs22I",
        "outputId": "16df881f-9256-4bf3-ae24-38471a211da9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_url\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_path\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jok7nzefqwC9"
      },
      "outputs": [],
      "source": [
        "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q2_K.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W5xe1O1Vq6nQ",
        "outputId": "03302d09-b97d-4728-988c-a500759e6ce3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /tmp/llama_index/models/llama-2-7b.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3904\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1952.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1952.00 MiB, K (f16):  976.00 MiB, V (f16):  976.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   283.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "query_llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url=model_url,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 1},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFzWRz9T9IP9",
        "outputId": "835d08b5-8815-4d68-e3d0-19309e1353bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaCPP(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7b5f82330d30>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7b5f822f1870>, completion_to_prompt=<function completion_to_prompt at 0x7b5f822f1900>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model_url='https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q2_K.gguf', model_path='/tmp/llama_index/models/llama-2-7b.Q2_K.gguf', temperature=0.1, max_new_tokens=256, context_window=3900, generate_kwargs={'temperature': 0.1, 'max_tokens': 256}, model_kwargs={'n_ctx': 3900, 'verbose': True, 'n_gpu_layers': 1}, verbose=True)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek1N8b7kI70u"
      },
      "outputs": [],
      "source": [
        "QUERY_GEN_PROMPT = (\n",
        "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
        "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
        "    \"related to the following input query:\\n\"\n",
        "    \"Query: {query}\\n\"\n",
        "    \"Queries:\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTvsGyG4JBPY"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core import Settings\n",
        "Settings.llm = remotely_run\n",
        "\n",
        "retriever = QueryFusionRetriever(\n",
        "    [index.as_retriever()],\n",
        "    llm=remotely_run,\n",
        "    similarity_top_k=2,\n",
        "    num_queries=4,  # set this to 1 to disable query generation\n",
        "    use_async=True,\n",
        "    verbose=True,\n",
        "    query_gen_prompt=QUERY_GEN_PROMPT,  # we could override the query generation prompt here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbKnZiLuJPDF"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvDQcM2_LE6a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYRtj_oyGiJ1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from IPython.display import Markdown, display\n",
        "qa_prompt_tmpl_str = \"\"\"\\\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Say Thank You at the end of answer.\n",
        "Please write the answer in the style of {tone_name}\n",
        "Query: {query_str}\n",
        "Answer: \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "partial_prompt_tmpl = prompt_tmpl.partial_format(tone_name=\"Doctor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHPOLG5OSNYy"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "synth = get_response_synthesizer(\n",
        "    text_qa_template=partial_prompt_tmpl\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4fdBR3zLwSv"
      },
      "outputs": [],
      "source": [
        "query = \"symptoms of anxiety\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4DU2CKmYMY5O",
        "outputId": "e9549c6b-4982-4377-eb1f-f0e8c6684da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated queries:\n",
            "1. common symptoms of anxiety\n",
            "2. physical symptoms of anxiety\n",
            "3. long-term effects of anxiety on the body\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[NodeWithScore(node=TextNode(id_='7374c783-7e9b-40c9-9812-4c77ec2d0785', embedding=None, metadata={'page_label': '397', 'file_name': 'the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_path': '/content/the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_type': 'application/pdf', 'file_size': 3909865, 'creation_date': '2024-08-29', 'last_modified_date': '2024-08-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c95198a5-5d25-4576-b150-002deaca725b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '397', 'file_name': 'the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_path': '/content/the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_type': 'application/pdf', 'file_size': 3909865, 'creation_date': '2024-08-29', 'last_modified_date': '2024-08-29'}, hash='4a4a073d0c28f2f8707b992bad7f4fbbad2a6c7619178362e26d47abd8ff9f8e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7911742c-24e7-479d-9446-3daa5aad1b1a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='80ae9058cd8920114bffcc3ce5e8a7c83bdebf5f115b6c881db7fc5bea79fcd8')}, text=\"1 Are\\tyou\\tbothered\\tby\\tmany\\tdifferent\\tsymptoms? □ □\\n2 Do\\tyou\\tfind\\tthat\\tyou\\tare\\toften\\taware\\tof\\tvarious\\tthings\\thappening\\tin\\tyour\\tbody? □ □\\n3 If\\ta\\tdisease\\tis\\tbrought\\tto\\tyour\\tattention\\tthrough\\tthe\\tradio,\\ttelevision,\\tnewspaper\\tor\\nsomeone\\tyou\\tknow,\\tdo\\tyou\\tworry\\tabout\\tgetting\\tit\\tyourself?□ □\\n4 Is\\tit\\teasy\\tfor\\tyou\\tto\\tforget\\tabout\\tyourself\\tand\\tthink\\tabout\\tall\\tsorts\\tof\\tother\\tthings? □ □\\n5 Do\\tyou\\tthink\\tthat\\tyou\\tworry\\tabout\\tyour\\thealth\\tmore\\tthan\\tmost\\tpeople? □ □\\n6 Are\\tyou\\tafraid\\tof\\tillness □ □\\n7 Is\\tit\\thard\\tfor\\tyou\\tto\\tbelieve\\tthe\\tdoctor\\twhen\\the\\ttells\\tyou\\tthere\\tis\\tnothing\\tfor\\tyou\\tto\\nworry\\tabout?□ □\\n8 Do\\tyou\\tget\\tthe\\tfeeling\\tthat\\tpeople\\tare\\tnot\\ttaking\\tyour\\tillness\\tseriously\\tenough? □ □\\n9 I've\\talways\\thad\\tone\\tthing\\tor\\tanother\\twrong\\twith\\tmy\\thealth. □ □\\n10 Others\\thave\\ttold\\tme\\tthat\\tI\\tspend\\ttoo\\tmuch\\ttime\\ttalking\\tabout\\tmy\\thealth. □ □\\n11 I\\tam\\tsure\\tsomething\\tis\\twrong\\twith\\tmy\\thealth\\tthat\\tthe\\tdoctors\\tstill\\thaven't\\tbeen\\nable\\tto\\tfind.□ □\\n12 I\\tfind\\tmyself\\tworrying\\tabout\\tmy\\thealth. □ □\\n13 I\\tfeel\\tlike\\tI\\tneed\\tto\\tsee\\tmy\\tphysician\\teven\\tthough\\tI\\tam\\tnot\\talways\\table\\tto. □ □\\nSection\\t4\\nYes No\\n1. Have\\tyou\\tbeen\\tdepressed\\tor\\tdown\\tin\\tyour\\tmood\\tor\\tlost\\tinterest\\tin\\tmost\\tthings\\nevery\\tday\\tfor\\tthe\\tpast\\ttwo\\tweeks?□ □\\n2.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1187, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7899901030649279),\n",
              " NodeWithScore(node=TextNode(id_='a236f8f5-2069-4fba-993f-2a7a2fe3c4a2', embedding=None, metadata={'page_label': '177', 'file_name': 'the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_path': '/content/the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_type': 'application/pdf', 'file_size': 3909865, 'creation_date': '2024-08-29', 'last_modified_date': '2024-08-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='61ba4d38-ad53-410e-9b0a-bc323c2e3fa1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '177', 'file_name': 'the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_path': '/content/the_nature_and_treatment_of_anxiety_disorders.pdf', 'file_type': 'application/pdf', 'file_size': 3909865, 'creation_date': '2024-08-29', 'last_modified_date': '2024-08-29'}, hash='bfeec040df3b44cbe4c30c53ebf129a7fdd85c906d901de55ea6b502eb6d1984'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='db2c41fc-23db-4a3a-885c-9a2c18aedcde', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5024c32f5916978c4caa662934f4ab7a1bc9fa58fe6523061b6621dfa8ef1ada')}, text='30\\t percent\\t of\\t \"normals”—\\t those\\t who\\t do\\t not\\t meet\\t the\\t criteria\\t for\\t an\\t anxiety\\ndisorder\\t and\\t have\\t not\\t sought\\t help\\t for\\t anxiety-related\\t difficulties—experience\\noccasional\\t episodes\\t of\\t panic\\t (Norton\\t et\\t al.,\\t 1985).\\t However,\\t normals\\t rarely,\\t if\\never,\\t experience\\t spontaneous\\t panic\\t attacks\\t with\\t as\\t many\\t as\\t four\\t or\\t more\\nsymptoms\\tand\\ta\\tseverity\\tof\\t4\\tor\\tgreater\\ton\\ta\\t0-10\\tscale.\\nTable\\t7.1\\tSymptoms\\tDuring\\tPanic\\nMeasurement\\nSymptomsConcurrentaRetrospectiveb\\nPalpitations 68% 100%\\nDizziness/lightheadedness 47 89\\nDyspnea 30 58\\nNausea/Abdominal\\tdistress 29 69\\nSweating 26 65\\nChest\\tpain/discomfort 25 65\\nFear\\tof\\tgoing\\tcrazy/doing\\tsomething\\tuncontrolled 21 73\\nTrembling/shaking 21 81\\nHot\\tflashes/cold\\tchills 17 73\\nDerealization/depersonalization 15 65\\nFaintness 10 69\\nParesthesias 10 46\\nChoking 5 35\\nFear\\tof\\tdying 3 31\\nAdapted\\tfrom\\tJ.\\tMargraf,\\tC.\\tB.\\tTaylor,\\tA.\\tEhlers,\\tT.\\tR.\\tWalton,\\tand\\tW.\\tS.\\tAgras,\\t\"Panic\\nAttacks\\t in\\t the\\t Natural\\t Environment,\"\\t The\\t Journal\\t of\\t Nervous\\t and\\t Mental\\t Disease,\\n175(9):558-565,\\t©\\tby\\tWilliams\\t&\\tWilkins,\\t1987.\\na\\tConcurrent\\trepresents\\tthe\\tpercentage\\tof\\t175\\tpanic\\tattacks\\twith\\tthis\\tsymptom.\\t\\nb\\tRetrospective\\trepresents\\tthe\\tpercentage\\tof\\t27\\tindividuals\\twho\\treported\\thaving\\tthe\\nsymptom\\tduring\\ta\\ttypical\\tpanic\\tattack.', mimetype='text/plain', start_char_idx=0, end_char_idx=1268, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7881797251300373)]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.retrieve(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nEMTkE9S1lP"
      },
      "outputs": [],
      "source": [
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": partial_prompt_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "UbhRmOhkLuC3",
        "outputId": "8ca636dc-5417-4378-9025-b3816d62518b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated queries:\n",
            "1. common symptoms of anxiety\n",
            "2. physical symptoms of anxiety\n",
            "3. long-term effects of anxiety on the body\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nDoctor: Thank you for your question. Anxiety disorders are characterized by excessive and persistent worry, fear, or apprehension about everyday situations. The symptoms of anxiety can vary from person to person, but some common symptoms include:\\n\\n1. Restlessness or feeling on edge\\n2. Difficulty concentrating or mind going blank\\n3. Irritability\\n4. Muscle tension\\n5. Sleep disturbances, such as difficulty falling asleep or staying asleep\\n6. Fatigue\\n7. Panic attacks, which can include symptoms such as chest pain, shortness of breath, and feelings of impending doom\\n8. Avoidance of situations that trigger anxiety\\n9. Excessive worry about health, money, or other issues\\n10. Obsessive thoughts or compulsive behaviors\\n\\nIf you have any further questions, please don't hesitate to ask. Thank you.\""
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_engine.query(query).response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "canGvsDzUG28"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.chat_engine import ContextChatEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nY5_6vjUp6l"
      },
      "outputs": [],
      "source": [
        "chat_engine = ContextChatEngine.from_defaults(\n",
        "    retriever = retriever,\n",
        "    memory = memory,\n",
        "    # chat_mode=\"context\",\n",
        "    # memory=memory,\n",
        "    system_prompt=(\n",
        "        \"You are a doctor. If you dont know the answer just say you dont know.\"\n",
        "    )\n",
        "    ),\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "SYNZxiLVVIvV",
        "outputId": "b6c7fd95-c970-415f-ffa5-51cac7fdc289"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'chat'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-f01aa7b7ca5a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'chat'"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\"Hello!\")\n",
        "response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0353c01cd89a4065bab8461ef6b463d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15739bf89beb4aee9d13554c908bcab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdc47b937eb4eb29debdd9e729a49d8",
            "placeholder": "​",
            "style": "IPY_MODEL_2a58b0f465c840ccb256c15dcaea3e0b",
            "value": " 5/5 [00:00&lt;00:00, 145.22it/s]"
          }
        },
        "2a58b0f465c840ccb256c15dcaea3e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cdc47b937eb4eb29debdd9e729a49d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fd4f58f33f64463b9dd6efad01ea8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7428364306a6468fb233872c8ac07947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f11ff3b037d04be4bdcd7c06f2bd5367",
              "IPY_MODEL_c7b9b89c104d4af9998dc0836f2ac35b",
              "IPY_MODEL_15739bf89beb4aee9d13554c908bcab2"
            ],
            "layout": "IPY_MODEL_5fd4f58f33f64463b9dd6efad01ea8f8"
          }
        },
        "9926be4517a34cf3a04a8d17c6bdc52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8296a056cd34b33b1b8b48072bb484d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7b9b89c104d4af9998dc0836f2ac35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8296a056cd34b33b1b8b48072bb484d",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9926be4517a34cf3a04a8d17c6bdc52f",
            "value": 5
          }
        },
        "ef95dfceb7ae4109ab1bccda41c1a27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f11ff3b037d04be4bdcd7c06f2bd5367": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0353c01cd89a4065bab8461ef6b463d5",
            "placeholder": "​",
            "style": "IPY_MODEL_ef95dfceb7ae4109ab1bccda41c1a27a",
            "value": "Fetching 5 files: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
